{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_4_Audio_Prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shsJj_FogIu0"
      },
      "source": [
        "# The Dataset:\n",
        "\n",
        "This project takes audio recordings of speakers saying the digits 0 - 9. My models predict which of the 6 speakers in the dataset is speaking. \n",
        "\n",
        "The dataset is called the \"Free Spoken Digit Dataset\". It includes 3,000 recordings, in .wav files, with minimal silence at the beginning and end of each recording.\n",
        "\n",
        "The data is available [here.](https://github.com/Jakobovski/free-spoken-digit-dataset)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z22T1lFF5j98"
      },
      "source": [
        "\n",
        "# Colab Set-Up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFUfNtr35En8"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpcobZ2T8aSQ"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7_LC79g5nLE"
      },
      "source": [
        "# global imports\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import dill\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from os import path\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score, KFold, train_test_split"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Hsao-uB5pJ9",
        "outputId": "3684db60-a2ab-444e-aa23-6716bdf2d621"
      },
      "source": [
        "# \"Mount\" the drive and then unzip it using the following code.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyOT7z6S7UKj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8f284d9c-5148-4a39-e258-afac679e31cc"
      },
      "source": [
        "# defining some global variables\n",
        "\n",
        "PATH = path.abspath(path.join(os.getcwd(), \"../..\"))\n",
        "\n",
        "dataset_name = \"recordings\"\n",
        "DATA_PATH = path.join(PATH, \"/content/drive/MyDrive/Colab Notebooks/ADV Machine Learning/Assignment 4 - Sound/{}\".format(dataset_name))\n",
        "\n",
        "DATA_PATH\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/ADV Machine Learning/Assignment 4 - Sound/recordings'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBP9sC3VlIz2"
      },
      "source": [
        "# add root directory to system path\n",
        "\n",
        "sys.path.append(PATH)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQvz6bCSlJ82"
      },
      "source": [
        "# initialize the set of labels from the speech dataset we are going to train our network\n",
        "# on\n",
        "\n",
        "CLASSES = [\"george\", \"nicolas\", \"theo\", \"yweweler\", \"jackson\", \"lucas\"]\n",
        "\n",
        "for cl in CLASSES:\n",
        "    assert path.exists(path.join(DATA_PATH, cl)), f\"Path does not exist for class {cl}\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er6D98c-1KeE"
      },
      "source": [
        "## Read Data: \n",
        "data = [] ## Set up list instead of Dict\n",
        "\n",
        "for cl in CLASSES:\n",
        "    # get list of files in a class directory\n",
        "    files = glob.glob(path.join(DATA_PATH, cl + \"/*\"))\n",
        "\n",
        "    for fp in files:\n",
        "        # load audio features using librosa\n",
        "        audio = librosa.load(fp, mono=True, sr=None)[0] # args read in raw audio data without conversions\n",
        "                                                        # Note that you can also read in audio to a specific length with \"duration\" argument\n",
        "        data.append((audio, cl))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq2yFmud5RL7",
        "outputId": "bab5fb4b-6942-473c-e865-e60f9567f352"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlpmd7So8f6r"
      },
      "source": [
        "# Visualize Data Set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXDnH1ku3QPc"
      },
      "source": [
        "### Visualizing the dataset (confirm load correctly)\n",
        "\n",
        "counts = [c for _, c in data]\n",
        "counts = [counts.count(c) for c in CLASSES]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "WJ_Y9Oem3XGd",
        "outputId": "08f35d1e-2557-4d2f-fd83-c7868cb113ec"
      },
      "source": [
        "plt.bar(CLASSES, counts, color=\"#c0e0e0\")\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7ddglGz4PJ3"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9dGN_J78kPW"
      },
      "source": [
        "## Define Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny4ZaY644REW"
      },
      "source": [
        "def preprocess(data, labels, one_hot_enc=None):\n",
        "    \"\"\"\n",
        "    This function preprocesses the the data to extract mfcc features for each audio\n",
        "    track and one-hot encodes all the labels\n",
        "    \n",
        "    params:\n",
        "        data\n",
        "            list of audio waves extracted using librosa\n",
        "        \n",
        "        labels\n",
        "            list of labels (integer or label names)\n",
        "            \n",
        "        one_hot_enc\n",
        "            object that has a method transform which transforms list of labels to their\n",
        "            one hot encoded form\n",
        "            \n",
        "    returns:\n",
        "        X\n",
        "            list of transformed features corresponding to data passed as input\n",
        "        \n",
        "        y\n",
        "            list of one-hot encoded labels\n",
        "        \n",
        "    \"\"\"\n",
        "\n",
        "    if one_hot_enc is None:\n",
        "        raise ValueError(\"one_hot_enc cannot be None\")\n",
        "\n",
        "    import librosa\n",
        "    import numpy as np\n",
        "\n",
        "    # fix length of audio tracks (pad if shorter, crop if longer)\n",
        "    X = np.array([librosa.util.fix_length(x, 16000) for x in data])  ## fix the lengths to make equal\n",
        "    X = np.array([librosa.feature.mfcc(x, sr=16000) for x in X]) #Mel filter cepstral coefficients (MFCC)\n",
        "\n",
        "## splits waveform up into different freq band\n",
        "## for each freq band, has different amplitudes \n",
        "## fourier (?) transform --> splits up waveform into sine waves \n",
        "## A bunch more steps happen \n",
        "## eventually use those as features \n",
        "\n",
        "    y = np.array([[l] for l in labels])\n",
        "    y = one_hot_enc.transform(y).toarray()\n",
        "\n",
        "    return X, y"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TS_Axsqx4mNM",
        "outputId": "3d82610d-5733-4f72-a9d2-9c642f49f660"
      },
      "source": [
        "# define the one-hot encoder\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "enc.fit(np.array([[cl] for cl in CLASSES]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder(handle_unknown='ignore')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3P4cNZI4pNB"
      },
      "source": [
        "## Pre-Process the Data: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUnJHiwt4rTi"
      },
      "source": [
        "# split the unprocessed data to covariates and labels\n",
        "\n",
        "data, labels = zip(*data) "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjcTrQFI4yMP",
        "outputId": "a13d532c-3274-4647-d324-3339ffb0c8b0"
      },
      "source": [
        "print(data[0].shape)\n",
        "print(labels[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2706,)\n",
            "george\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPf_Yz9956Rm"
      },
      "source": [
        "# preprocess the data to be used later\n",
        "\n",
        "X, y = preprocess(data, labels,enc)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzERefC66Qwi",
        "outputId": "0e03b784-7168-4d50-e251-fdc5dd14bed0"
      },
      "source": [
        "X.shape # our mfcc transformation outputs to 32 x input features"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 20, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fc8hnW76BXR",
        "outputId": "0b27587f-d21d-4e73-9ac1-974e0f1eed5c"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfDcKTx06cH_"
      },
      "source": [
        "## Define Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C3Vt6yU6eqV"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUHwv-EH6ij8"
      },
      "source": [
        "# Predictive Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkIcFwU28rrp"
      },
      "source": [
        "## Model #1: Basic LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lendTK1m6kW4"
      },
      "source": [
        "# Load libraries\n",
        "import sys\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from tensorflow.python.keras.models import Sequential, Model\n",
        "from tensorflow.python.keras.layers import LSTM,Dense, Dropout, Flatten, Activation, BatchNormalization\n",
        "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D \n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "#from tensorflow.python.keras.optimizers import Adagrad,Adadelta,RMSprop\n",
        "\n",
        "\n",
        "\n",
        "# defining the model\n",
        "dense_out_3 = len(CLASSES)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(200))\n",
        "model.add(Dense(20, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"relu\"))\n",
        "model.add(Dense(dense_out_3, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXNy7EDN7M65",
        "outputId": "6110d16c-2311-49ec-cc4c-3ef17c5b6c78"
      },
      "source": [
        "with tf.device('/device:GPU:0'): \n",
        "\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=20,\n",
        "        batch_size=72,\n",
        "        validation_data=(X_test, y_test),\n",
        "        verbose=2,\n",
        "        shuffle=True,\n",
        "    )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "30/30 - 7s - loss: 1.3205 - accuracy: 0.5752 - val_loss: 0.8834 - val_accuracy: 0.7211\n",
            "Epoch 2/20\n",
            "30/30 - 4s - loss: 0.6164 - accuracy: 0.8176 - val_loss: 0.4197 - val_accuracy: 0.8944\n",
            "Epoch 3/20\n",
            "30/30 - 4s - loss: 0.2453 - accuracy: 0.9410 - val_loss: 0.2293 - val_accuracy: 0.9300\n",
            "Epoch 4/20\n",
            "30/30 - 4s - loss: 0.1066 - accuracy: 0.9729 - val_loss: 0.1786 - val_accuracy: 0.9478\n",
            "Epoch 5/20\n",
            "30/30 - 4s - loss: 0.0495 - accuracy: 0.9890 - val_loss: 0.1829 - val_accuracy: 0.9467\n",
            "Epoch 6/20\n",
            "30/30 - 4s - loss: 0.0472 - accuracy: 0.9895 - val_loss: 0.1513 - val_accuracy: 0.9533\n",
            "Epoch 7/20\n",
            "30/30 - 4s - loss: 0.0172 - accuracy: 0.9990 - val_loss: 0.1157 - val_accuracy: 0.9689\n",
            "Epoch 8/20\n",
            "30/30 - 4s - loss: 0.0083 - accuracy: 0.9995 - val_loss: 0.1164 - val_accuracy: 0.9656\n",
            "Epoch 9/20\n",
            "30/30 - 4s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.1243 - val_accuracy: 0.9656\n",
            "Epoch 10/20\n",
            "30/30 - 4s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1241 - val_accuracy: 0.9667\n",
            "Epoch 11/20\n",
            "30/30 - 4s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.1223 - val_accuracy: 0.9678\n",
            "Epoch 12/20\n",
            "30/30 - 4s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1280 - val_accuracy: 0.9644\n",
            "Epoch 13/20\n",
            "30/30 - 4s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1282 - val_accuracy: 0.9656\n",
            "Epoch 14/20\n",
            "30/30 - 4s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1278 - val_accuracy: 0.9667\n",
            "Epoch 15/20\n",
            "30/30 - 4s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1328 - val_accuracy: 0.9644\n",
            "Epoch 16/20\n",
            "30/30 - 4s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1335 - val_accuracy: 0.9656\n",
            "Epoch 17/20\n",
            "30/30 - 4s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1320 - val_accuracy: 0.9656\n",
            "Epoch 18/20\n",
            "30/30 - 4s - loss: 9.2871e-04 - accuracy: 1.0000 - val_loss: 0.1326 - val_accuracy: 0.9656\n",
            "Epoch 19/20\n",
            "30/30 - 4s - loss: 8.2353e-04 - accuracy: 1.0000 - val_loss: 0.1362 - val_accuracy: 0.9644\n",
            "Epoch 20/20\n",
            "30/30 - 4s - loss: 7.3572e-04 - accuracy: 1.0000 - val_loss: 0.1370 - val_accuracy: 0.9667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63UTqo6n-XZ-"
      },
      "source": [
        "## Model #2: Stacked LSTMs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjCXJ9RiAEo7"
      },
      "source": [
        "model2 = Sequential()\n",
        "model2.add(LSTM(200, return_sequences=True))\n",
        "model2.add(LSTM(200, return_sequences=True))\n",
        "model2.add(LSTM(200))\n",
        "model2.add(Dense(20, activation=\"relu\"))\n",
        "model2.add(Dense(10, activation=\"relu\"))\n",
        "model2.add(Dense(dense_out_3, activation=\"softmax\"))\n",
        "\n",
        "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmkTZzjMAPMU",
        "outputId": "55842211-b4d8-451b-b8c7-7d7ea405b5bb"
      },
      "source": [
        "with tf.device('/device:GPU:0'): \n",
        "\n",
        "    history = model2.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=20,\n",
        "        batch_size=72,\n",
        "        validation_data=(X_test, y_test),\n",
        "        verbose=2,\n",
        "        shuffle=True,\n",
        "    )"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "30/30 - 24s - loss: 1.1802 - accuracy: 0.5914 - val_loss: 0.8098 - val_accuracy: 0.7411\n",
            "Epoch 2/20\n",
            "30/30 - 14s - loss: 0.6289 - accuracy: 0.7895 - val_loss: 0.4892 - val_accuracy: 0.8800\n",
            "Epoch 3/20\n",
            "30/30 - 14s - loss: 0.3726 - accuracy: 0.9071 - val_loss: 0.5422 - val_accuracy: 0.8367\n",
            "Epoch 4/20\n",
            "30/30 - 14s - loss: 0.2060 - accuracy: 0.9467 - val_loss: 0.2705 - val_accuracy: 0.9233\n",
            "Epoch 5/20\n",
            "30/30 - 14s - loss: 0.0969 - accuracy: 0.9733 - val_loss: 0.2151 - val_accuracy: 0.9456\n",
            "Epoch 6/20\n",
            "30/30 - 14s - loss: 0.0448 - accuracy: 0.9886 - val_loss: 0.2925 - val_accuracy: 0.9211\n",
            "Epoch 7/20\n",
            "30/30 - 14s - loss: 0.0712 - accuracy: 0.9814 - val_loss: 0.2039 - val_accuracy: 0.9522\n",
            "Epoch 8/20\n",
            "30/30 - 14s - loss: 0.0323 - accuracy: 0.9924 - val_loss: 0.1458 - val_accuracy: 0.9656\n",
            "Epoch 9/20\n",
            "30/30 - 14s - loss: 0.0448 - accuracy: 0.9881 - val_loss: 0.3492 - val_accuracy: 0.9122\n",
            "Epoch 10/20\n",
            "30/30 - 14s - loss: 0.0504 - accuracy: 0.9871 - val_loss: 0.1516 - val_accuracy: 0.9600\n",
            "Epoch 11/20\n",
            "30/30 - 14s - loss: 0.0351 - accuracy: 0.9900 - val_loss: 0.2130 - val_accuracy: 0.9511\n",
            "Epoch 12/20\n",
            "30/30 - 15s - loss: 0.0139 - accuracy: 0.9962 - val_loss: 0.1279 - val_accuracy: 0.9678\n",
            "Epoch 13/20\n",
            "30/30 - 14s - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.1252 - val_accuracy: 0.9711\n",
            "Epoch 14/20\n",
            "30/30 - 14s - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.1522 - val_accuracy: 0.9756\n",
            "Epoch 15/20\n",
            "30/30 - 14s - loss: 9.4837e-04 - accuracy: 1.0000 - val_loss: 0.1476 - val_accuracy: 0.9744\n",
            "Epoch 16/20\n",
            "30/30 - 14s - loss: 7.2310e-04 - accuracy: 1.0000 - val_loss: 0.1509 - val_accuracy: 0.9744\n",
            "Epoch 17/20\n",
            "30/30 - 14s - loss: 6.0020e-04 - accuracy: 1.0000 - val_loss: 0.1532 - val_accuracy: 0.9744\n",
            "Epoch 18/20\n",
            "30/30 - 14s - loss: 5.1630e-04 - accuracy: 1.0000 - val_loss: 0.1549 - val_accuracy: 0.9756\n",
            "Epoch 19/20\n",
            "30/30 - 14s - loss: 4.5264e-04 - accuracy: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.9756\n",
            "Epoch 20/20\n",
            "30/30 - 14s - loss: 4.0121e-04 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 0.9744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nIWLPbt-Yd4"
      },
      "source": [
        "## Model #3: Add Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN6KaRhPBEQz"
      },
      "source": [
        "model3 = Sequential()\n",
        "model3.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "model3.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "model3.add(LSTM(200))\n",
        "model3.add(Dense(20, activation=\"relu\"))\n",
        "model3.add(Dense(10, activation=\"relu\"))\n",
        "model3.add(Dense(dense_out_3, activation=\"softmax\"))\n",
        "\n",
        "model3.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUZygs5BBGO3",
        "outputId": "443f0190-d20e-42f7-8587-c64304a9ec27"
      },
      "source": [
        "with tf.device('/device:GPU:0'): \n",
        "\n",
        "    history = model3.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=20,\n",
        "        batch_size=72,\n",
        "        validation_data=(X_test, y_test),\n",
        "        verbose=2,\n",
        "        shuffle=True,\n",
        "    )\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "30/30 - 27s - loss: 1.2833 - accuracy: 0.4690 - val_loss: 0.9532 - val_accuracy: 0.6478\n",
            "Epoch 2/20\n",
            "30/30 - 15s - loss: 0.7240 - accuracy: 0.6771 - val_loss: 0.5965 - val_accuracy: 0.7333\n",
            "Epoch 3/20\n",
            "30/30 - 15s - loss: 0.5190 - accuracy: 0.8076 - val_loss: 0.4905 - val_accuracy: 0.8622\n",
            "Epoch 4/20\n",
            "30/30 - 16s - loss: 0.3378 - accuracy: 0.9105 - val_loss: 0.4305 - val_accuracy: 0.8567\n",
            "Epoch 5/20\n",
            "30/30 - 15s - loss: 0.2270 - accuracy: 0.9305 - val_loss: 0.2686 - val_accuracy: 0.9311\n",
            "Epoch 6/20\n",
            "30/30 - 15s - loss: 0.2217 - accuracy: 0.9333 - val_loss: 0.3310 - val_accuracy: 0.9200\n",
            "Epoch 7/20\n",
            "30/30 - 16s - loss: 0.1900 - accuracy: 0.9462 - val_loss: 0.2288 - val_accuracy: 0.9333\n",
            "Epoch 8/20\n",
            "30/30 - 15s - loss: 0.1229 - accuracy: 0.9619 - val_loss: 0.2199 - val_accuracy: 0.9456\n",
            "Epoch 9/20\n",
            "30/30 - 16s - loss: 0.1567 - accuracy: 0.9486 - val_loss: 0.2108 - val_accuracy: 0.9411\n",
            "Epoch 10/20\n",
            "30/30 - 16s - loss: 0.1338 - accuracy: 0.9552 - val_loss: 0.1728 - val_accuracy: 0.9622\n",
            "Epoch 11/20\n",
            "30/30 - 15s - loss: 0.0896 - accuracy: 0.9733 - val_loss: 0.1453 - val_accuracy: 0.9622\n",
            "Epoch 12/20\n",
            "30/30 - 15s - loss: 0.0679 - accuracy: 0.9795 - val_loss: 0.1208 - val_accuracy: 0.9689\n",
            "Epoch 13/20\n",
            "30/30 - 16s - loss: 0.0737 - accuracy: 0.9805 - val_loss: 0.2415 - val_accuracy: 0.9467\n",
            "Epoch 14/20\n",
            "30/30 - 15s - loss: 0.0470 - accuracy: 0.9886 - val_loss: 0.2128 - val_accuracy: 0.9500\n",
            "Epoch 15/20\n",
            "30/30 - 16s - loss: 0.0441 - accuracy: 0.9881 - val_loss: 0.1914 - val_accuracy: 0.9444\n",
            "Epoch 16/20\n",
            "30/30 - 15s - loss: 0.0560 - accuracy: 0.9800 - val_loss: 0.1157 - val_accuracy: 0.9767\n",
            "Epoch 17/20\n",
            "30/30 - 16s - loss: 0.0653 - accuracy: 0.9852 - val_loss: 0.1335 - val_accuracy: 0.9689\n",
            "Epoch 18/20\n",
            "30/30 - 15s - loss: 0.0323 - accuracy: 0.9910 - val_loss: 0.1129 - val_accuracy: 0.9767\n",
            "Epoch 19/20\n",
            "30/30 - 15s - loss: 0.0533 - accuracy: 0.9838 - val_loss: 0.1806 - val_accuracy: 0.9522\n",
            "Epoch 20/20\n",
            "30/30 - 15s - loss: 0.0473 - accuracy: 0.9848 - val_loss: 0.1962 - val_accuracy: 0.9567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO2dQ7rq-ZbL"
      },
      "source": [
        "## Model #4: Add Bidirectional Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByOq_pRNCoLX"
      },
      "source": [
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "model4 = Sequential()\n",
        "model4.add(Bidirectional(LSTM(200, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))\n",
        "model4.add(LSTM(200, dropout=0.25, recurrent_dropout=0.25, return_sequences=True))\n",
        "model4.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2 ))\n",
        "model4.add(Dense(20, activation=\"relu\"))\n",
        "model4.add(Dense(10, activation=\"relu\"))\n",
        "model4.add(Dense(dense_out_3, activation=\"softmax\"))\n",
        "\n",
        "model4.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTT6emzSCpD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d80272f2-be2a-46ae-f697-e14087563af8"
      },
      "source": [
        "checkpoint_filepath = '/tmp/checkpoint'\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "# Model weights are saved at the end of every epoch, if it's the best seen so far.\n",
        "\n",
        "with tf.device('/device:GPU:0'): \n",
        "\n",
        "    history = model4.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=25,\n",
        "        batch_size=72,\n",
        "        validation_data=(X_test, y_test),\n",
        "        verbose=2,\n",
        "        shuffle=True,\n",
        "        callbacks=[model_checkpoint_callback],\n",
        "    )\n",
        "\n",
        "#Load best weights into model\n",
        "model4.load_weights(checkpoint_filepath)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "30/30 - 41s - loss: 1.1857 - accuracy: 0.5205 - val_loss: 0.8300 - val_accuracy: 0.7300\n",
            "Epoch 2/25\n",
            "30/30 - 24s - loss: 0.6170 - accuracy: 0.7967 - val_loss: 0.4705 - val_accuracy: 0.8367\n",
            "Epoch 3/25\n",
            "30/30 - 24s - loss: 0.4005 - accuracy: 0.8581 - val_loss: 0.3686 - val_accuracy: 0.8778\n",
            "Epoch 4/25\n",
            "30/30 - 23s - loss: 0.2775 - accuracy: 0.9071 - val_loss: 0.2764 - val_accuracy: 0.9056\n",
            "Epoch 5/25\n",
            "30/30 - 23s - loss: 0.2292 - accuracy: 0.9243 - val_loss: 0.1977 - val_accuracy: 0.9300\n",
            "Epoch 6/25\n",
            "30/30 - 24s - loss: 0.1719 - accuracy: 0.9462 - val_loss: 0.2633 - val_accuracy: 0.9233\n",
            "Epoch 7/25\n",
            "30/30 - 23s - loss: 0.1638 - accuracy: 0.9476 - val_loss: 0.1108 - val_accuracy: 0.9644\n",
            "Epoch 8/25\n",
            "30/30 - 24s - loss: 0.1432 - accuracy: 0.9510 - val_loss: 0.1260 - val_accuracy: 0.9556\n",
            "Epoch 9/25\n",
            "30/30 - 23s - loss: 0.1107 - accuracy: 0.9633 - val_loss: 0.1316 - val_accuracy: 0.9533\n",
            "Epoch 10/25\n",
            "30/30 - 23s - loss: 0.0657 - accuracy: 0.9771 - val_loss: 0.0854 - val_accuracy: 0.9700\n",
            "Epoch 11/25\n",
            "30/30 - 23s - loss: 0.0574 - accuracy: 0.9829 - val_loss: 0.1753 - val_accuracy: 0.9467\n",
            "Epoch 12/25\n",
            "30/30 - 23s - loss: 0.0902 - accuracy: 0.9710 - val_loss: 0.1371 - val_accuracy: 0.9633\n",
            "Epoch 13/25\n",
            "30/30 - 24s - loss: 0.0543 - accuracy: 0.9838 - val_loss: 0.0814 - val_accuracy: 0.9756\n",
            "Epoch 14/25\n",
            "30/30 - 24s - loss: 0.0508 - accuracy: 0.9829 - val_loss: 0.1382 - val_accuracy: 0.9678\n",
            "Epoch 15/25\n",
            "30/30 - 23s - loss: 0.0673 - accuracy: 0.9776 - val_loss: 0.1191 - val_accuracy: 0.9622\n",
            "Epoch 16/25\n",
            "30/30 - 23s - loss: 0.0574 - accuracy: 0.9814 - val_loss: 0.1721 - val_accuracy: 0.9578\n",
            "Epoch 17/25\n",
            "30/30 - 23s - loss: 0.0610 - accuracy: 0.9786 - val_loss: 0.1152 - val_accuracy: 0.9533\n",
            "Epoch 18/25\n",
            "30/30 - 23s - loss: 0.0527 - accuracy: 0.9862 - val_loss: 0.1477 - val_accuracy: 0.9544\n",
            "Epoch 19/25\n",
            "30/30 - 23s - loss: 0.0458 - accuracy: 0.9829 - val_loss: 0.0837 - val_accuracy: 0.9744\n",
            "Epoch 20/25\n",
            "30/30 - 24s - loss: 0.0224 - accuracy: 0.9924 - val_loss: 0.1068 - val_accuracy: 0.9756\n",
            "Epoch 21/25\n",
            "30/30 - 24s - loss: 0.0642 - accuracy: 0.9776 - val_loss: 0.0900 - val_accuracy: 0.9744\n",
            "Epoch 22/25\n",
            "30/30 - 23s - loss: 0.0488 - accuracy: 0.9848 - val_loss: 0.1181 - val_accuracy: 0.9711\n",
            "Epoch 23/25\n",
            "30/30 - 24s - loss: 0.0275 - accuracy: 0.9919 - val_loss: 0.0738 - val_accuracy: 0.9789\n",
            "Epoch 24/25\n",
            "30/30 - 24s - loss: 0.0356 - accuracy: 0.9914 - val_loss: 0.0949 - val_accuracy: 0.9744\n",
            "Epoch 25/25\n",
            "30/30 - 24s - loss: 0.0279 - accuracy: 0.9910 - val_loss: 0.0954 - val_accuracy: 0.9744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fde14369310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oMuAIuCbZqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "107179e1-207b-4bbf-beae-bf8dc47f2d70"
      },
      "source": [
        "model4.evaluate(X_test, y_test)\n",
        "# confirm loaded weights"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29/29 [==============================] - 2s 72ms/step - loss: 0.0738 - accuracy: 0.9789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07383837550878525, 0.9788888692855835]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ldl5Vly-dwG"
      },
      "source": [
        "# Reflection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpyouJR_EqEn"
      },
      "source": [
        "The best performing model was Model 4, which had one bidirectional LSTM layer, two regular LSTM layers, and 3 dense layers. It had a validation accuracy of 97.89%. \n",
        "\n",
        "The worst performing model was Model 3, which had three stacked LSTM layers, two of which had dropout, and 3 dense layers. It had an a validation accuracy of 95.67%. \n",
        "\n",
        "All things considered, if I had to choose one model to use in production, I would probably go with Model 2. It's validation accuracy was comparable to model 4 (97.44% as compared to 97.89%), but it was a lot faster to train because the LSTM layers didn't have dropout, and it is marginally simpler because there are no bidirectional layers. "
      ]
    }
  ]
}